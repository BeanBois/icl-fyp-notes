HVT: A COMPREHENSIVE VISION FRAMEWORK FOR LEARNING IN NON-EUCLIDEAN SPACE

Many real-world datasets (images) exhibit hierarchical structures better captured in non-euclidean spaces
Images are inherently hierarchical, building from pixels, to edges, to shapes, to objects and finally to scene
    This hierarchical nature reflects a tree-like structure 
    > Vision Transformers (ViTs) uses a patch-based approach to introduce a hierarchical representation 
        - It does so as:
            1: each patch captures local patterns like textures/edges
            2: By attending over patches, the model aggregates local information to uinderstand the overall structure 
    Hyperbolic spaces are well-suited for modelling hierarchical data due to their ability to embed tree-like structures with minimal distortion 
        By using hyperbolic geometry and Mobius transformations, we can effectively capture the multi-scae dependencies inherent in visual data
        Specifically, Mobius transofrmation allow for operations like addition and scalar multiplication in hyperbolic space, enabling neural netowrks to perform calculations wihle preserving hierarchical r/s 


Related Studies 
    - Hyperbolic Geometry in Machine Learning
        > Nicker & Kiela sucessfully used Poincare embeddings to model hierichical data [Poincare]
        > This was further refined by Ganea 2018, who introduced hyperbolic emb for entailment cones, effectively capturing asymmetric r/s
        > Khrulkov 2020, Liu 2020 extended these concepts to visual data and zero-shot recognition, respectively, underscoring the versatility of hyperbolic embeddings in handling 
        complex visual tasks
        > Ermolov (2022) developed Hyperbolic Vision Transformers that incorporate these embeddings within Vision Transformer architectures, enhancing metric learning.
        > Our model extends on HVTR by incorporating hyperbolic geometry throughout the transformer operations, from Mobious transformations to hyperbolic self-attention

    - Hyperbolic Nerual Network and attention
        > Ganea 2018 formualted HNN by introducing layers and activation functions suited for hyperbolic spaces
        > This was expanded by Bachmann 2020 who focues on optimising these networks efficiently 
        > Our model enriches HNN by embedding hyperbolic layers directly within transformer architectures, enhancing adaptibility and depth of hyperbolic ops.
        > Hyperbolic Attention Networks by Gulchehe 2018 is an alternative method from our model
            - It differs primarily in how  hyperbolic geometry is applied within the attention mechanism
            - HAN focuses on embedding te activations into hyperbolic space using both the hyperboloid and Klein models, leveraging hyperbolic matching and aggregation operations
            - Our model incorporates learnable curvature within positional embeddings, head-specific scaling in attention, and hyperbolic layer normalization, offering more flexibility and 
            efficiency in capturing hierarchical data 
                > Our model also utilises Poincare ball model for its computation suitability in vision tasks
    
    - Vision Transformers 
        > Dosovitskiy 2021b adapted Vision Transformer for visual tasks
        > Enhancements by Caron 2021 and El-Nouby 2021 have refined ViTs for self-supervised learning 
        > However these model are Euclidean-based, and our HVT employs hyperbolic geometry to model hierarchical and relational data structures more effectively 

    - Comparison to Key Hyperbolic Methods
        > Ermolov 2022 
            - Their HVTs focuses on hyperbolic embeddings for metric learning, while our approach full integrates hyperbolic operations throughout the transformers 
            - This significantly enhance its ability to manage hierarchical data beyond the objectives of metric learning
            - We extend the application scope to include image calssification by embedding hyperbolic geometry directly into the core compents of the Vision Transformer
        
        > Yang 2024, Hypformer
            - Hypformer is an efficient hyperbolic Transformer based on the Lorentz model of hyperbolic geometry
            - Hypformer introduces 2 foundational blocks to define essential Transformer modules in hyperbolic space 
                1: Hyperbolic transformation with Curvatures 
                2: Hyperbolic Readjustments and Refinement with Curvatures 
            - A linear self-attention mechanism in hyperbolic space is also developed to handle large-scale graph data and long-sequence inputs efficiently
            - Our model differs in several key aspects: 
                1: Our model focus on vision tasks (esp image classification), integrating hyperbolic geometry throughout the Vision Transformer architecture to enchance the modeling of hierarchical and relational structures inherent in visual data 
                    > Hypformer is mainly designed for graph data and emphasises on scalability and efficiency in handling large-scale graphs and long sequences 
                2: Our model utilises the Poincare ball model, which is advantageous for vision tasks for its conformal properties, which preserves angles and better represent geometric structure in image data
                    > Hypformer operates in Lorentz model of hyperbolic geometry 
                3: Our model introduces learnable curvature in positional embeddings, head-specific scaling in the attention mechanism, hyperbolic layer normalisation, gradient clipping, geodesic regularisation and layer scaling for training stability. These are specifc for Vision Transformers 
                4: Our model extends the standard self-attention mechanism into hyperbolic space using mobius operations and hyperbolic distance calculations to allow usnto capture complex relationship in visual data more effectively 

Methodology
    - Code Availability: Code is available at https://github.com/hyperbolicvit/hyperbolicvit

    - Hyperbolic Geometry Preliminaries
        > Hyperbolic space is chraracterised by constant negative curvature and it embeds hierarchical and complex structures common in visual data
        > Poncare ball model is used due to its computational convenience and suitablility for representing image data structures
        > Poincare ball model:
            - The n-dimensional Poincare ball model is defined as the manifold:
                D^n = {x in R | |x| < 1}, |.| is euclidean norm 
            - The Riemannian metric tensor g_x of this manifold is given by:
                g_x = (l_x)^2 g^E , with l_x = 2/( 1 - |x|^2 )
                where g^E is the Euclidean metric tensor, and the l_xis the conformal factor that scales the Euclidean metric to account for the curvature of hyperbolic space
        > Mobius Operations
            - Mobius transformations is utilised to adapt Vision Transformer components to hyperbolic space
            - These operations are essentual for processing vectors within the Poincare ball model
            1: Mobius Addition
                - For vectors x,y in D^n, the mobius addition x + y is defined as 
                    > x + y = \frac{(1+ 2<x,y> + ||y||^2 )x + (1 - ||x||^2)y}{1 + 2<x,y> + ||x||^2 ||y||^2}
                    > <.,.> denotes Euclidean inner product 
                        - For vectors x =  (x_1 ... x_n) and y = (y_1 ... y_n), <x,y> = sum[i=1:n] (x_i)(y_i) = x . y (dot product)
                        - The inner product encodes fundamental geometric concepts:
                            1: Length/Norm: ||u|| = √⟨u, u⟩
                            2: Angle: cos θ = ⟨u, v⟩ / (||u|| ||v||)
                            3: Orthogonality: u ⊥ v if and only if ⟨u, v⟩ = 0
                            4: Distance: d(u, v) = ||u - v||
                - Properties:
                    1: if |x| < 1 & |y| < 1, then |x+y| < 1 
                    2: Commutative : x + y == y + x
                    3: Associative: (a + b) + c == a + (b + c)
                    4: Identity Element 0: a + 0 = a 
                    5: Inverse of a is -a : a + -a = 0 
                - Mobius addition generalises vector additioni to hyperbolic space, ensuring the result remains within manifold 
                - Physical Interpretation:
                    > In special relativity, In special relativity, if you have two velocities v₁ and v₂ (as fractions of light speed c), their relativistic sum isn't simply v₁ + v₂. 
                    Instead, it follows the Mobius addition formula, ensuring the combined velocity never exceeds the speed of light.
                - Mobius Addition naturally emerges when working with distances and transformations in Poincaré disk model
            2: Mobius Scalar Mulitplication 
                - For a scalar r in R and a vector x in D^n, Mobius scalar mult r * x can be defined as 
                    > r * x = tanh( r tanh^-1 ( ||x|| ) ) \frac{x}{||x||}
                - Properties:
                    1: Distributivity over Mob Addition: r * (a + b) = (r * a) + (r * b)
                    2: Associativivity with real multiplication: (rs) * a = r * (s * a)
                    3: Identity Element 1: 1 * a = a 
                    4: Zero Annihilation: 0 * a = 0
                    5: Sign Behaviour: a * -1 = -a (Mobius Inverse) 
                - This operation scales a vector while preserving its direction and ensures the scaled vector remains within the Poincare ball.
                - In the Poincaré disk model, Mobius scalar multiplication corresponds to scaling along hyperbolic rays from the origin. The operation:
                    1: Preserves the direction of hyperbolic geodesics through the origin
                    2: Scales hyperbolic distances in a non-linear way
                    3: Maintains the conformal structure of the hyperbolic plane
            3: Mobius Matrix-Vector Mulitplication
                - Given a matrix W in R^(m x n) and a vector x in D^n, Mobius matrix-vector multiplication mv_mult(W, x) is defined as:
                    > mv_mult(W, x) = tanh( \frac{||Wx||}{||x||} tanh^-1(||x||) ) \frac{Wx}{||Wx||}, if x != 0 
                    > Or mv_mult(W,x) = (W_1,1 * x_1) + ... (W_1,n * x_n)
                - Mobius matrix-vector multiplication extends matrix-vector operations to hyperbolic geometry, providing a way to perform linear transformations in non-Euclidean spaces while 
                preserving the underlying hyperbolic structure.
                - This operation extends Mobius scalar multiplication to linear transformations, allowing us to apply linear layers within hyperbolic space 

            4: Mobius Concatenation
                - To combine multiple vectors x_1, ... x_n in D^n, we use Mobius concatenation:
                    MULT[j=1:n]x_j = x_1 * x_2 ... * x_n 

        - Hyperbolic Neural Network Components 
            > Mobius operations is incorporated into neural network components to enable ViTs to operate within hyperbolic space
            1: Hyperbolic Linear Layers 
                - Traditional layers (h = Wx + b) can be adapted to hyperbolic space using Mobius matrix-vector multiplication and addition:
                    > h = mv_mult(W,x) + b
                    > x in D^n, W in R^(m x n), b in D^m
            2: Hyperbolic Activation and Normalisation
                saf
