HVT: A COMPREHENSIVE VISION FRAMEWORK FOR LEARNING IN NON-EUCLIDEAN SPACE

Many real-world datasets (images) exhibit hierarchical structures better captured in non-euclidean spaces
Images are inherently hierarchical, building from pixels, to edges, to shapes, to objects and finally to scene
    This hierarchical nature reflects a tree-like structure 
    > Vision Transformers (ViTs) uses a patch-based approach to introduce a hierarchical representation 
        - It does so as:
            1: each patch captures local patterns like textures/edges
            2: By attending over patches, the model aggregates local information to uinderstand the overall structure 
    Hyperbolic spaces are well-suited for modelling hierarchical data due to their ability to embed tree-like structures with minimal distortion 
        By using hyperbolic geometry and Mobius transformations, we can effectively capture the multi-scae dependencies inherent in visual data
        Specifically, Mobius transofrmation allow for operations like addition and scalar multiplication in hyperbolic space, enabling neural netowrks to perform calculations wihle preserving hierarchical r/s 


Related Studies 
    - Hyperbolic Geometry in Machine Learning
        > Nicker & Kiela sucessfully used Poincare embeddings to model hierichical data [Poincare]
        > This was further refined by Ganea 2018, who introduced hyperbolic emb for entailment cones, effectively capturing asymmetric r/s
        > Khrulkov 2020, Liu 2020 extended these concepts to visual data and zero-shot recognition, respectively, underscoring the versatility of hyperbolic embeddings in handling 
        complex visual tasks
        > Ermolov (2022) developed Hyperbolic Vision Transformers that incorporate these embeddings within Vision Transformer architectures, enhancing metric learning.
        > Our model extends on HVTR by incorporating hyperbolic geometry throughout the transformer operations, from Mobious transformations to hyperbolic self-attention

    - Hyperbolic Nerual Network and attention
        > Ganea 2018 formualted HNN by introducing layers and activation functions suited for hyperbolic spaces
        > This was expanded by Bachmann 2020 who focues on optimising these networks efficiently 
        > Our model enriches HNN by embedding hyperbolic layers directly within transformer architectures, enhancing adaptibility and depth of hyperbolic ops.
        > Hyperbolic Attention Networks by Gulchehe 2018 is an alternative method from our model
            - It differs primarily in how  hyperbolic geometry is applied within the attention mechanism
            - HAN focuses on embedding te activations into hyperbolic space using both the hyperboloid and Klein models, leveraging hyperbolic matching and aggregation operations
            - Our model incorporates learnable curvature within positional embeddings, head-specific scaling in attention, and hyperbolic layer normalization, offering more flexibility and 
            efficiency in capturing hierarchical data 
                > Our model also utilises Poincare ball model for its computation suitability in vision tasks
    
    - Vision Transformers 
        > Dosovitskiy 2021b adapted Vision Transformer for visual tasks
        > Enhancements by Caron 2021 and El-Nouby 2021 have refined ViTs for self-supervised learning 
        > However these model are Euclidean-based, and our HVT employs hyperbolic geometry to model hierarchical and relational data structures more effectively 

    - Comparison to Key Hyperbolic Methods
        > Ermolov 2022 
            - Their HVTs focuses on hyperbolic embeddings for metric learning, while our approach full integrates hyperbolic operations throughout the transformers 
            - This significantly enhance its ability to manage hierarchical data beyond the objectives of metric learning
            - We extend the application scope to include image calssification by embedding hyperbolic geometry directly into the core compents of the Vision Transformer
        
        > Yang 2024, Hypformer
            - Hypformer is an efficient hyperbolic Transformer based on the Lorentz model of hyperbolic geometry
            - Hypformer introduces 2 foundational blocks to define essential Transformer modules in hyperbolic space 
                1: Hyperbolic transformation with Curvatures 
                2: Hyperbolic Readjustments and Refinement with Curvatures 
            - A linear self-attention mechanism in hyperbolic space is also developed to handle large-scale graph data and long-sequence inputs efficiently
            - Our model differs in several key aspects: 
                1: Our model focus on vision tasks (esp image classification), integrating hyperbolic geometry throughout the Vision Transformer architecture to enchance the modeling of hierarchical and relational structures inherent in visual data 
                    > Hypformer is mainly designed for graph data and emphasises on scalability and efficiency in handling large-scale graphs and long sequences 
                2: Our model utilises the Poincare ball model, which is advantageous for vision tasks for its conformal properties, which preserves angles and better represent geometric structure in image data
                    > Hypformer operates in Lorentz model of hyperbolic geometry 
                3: Our model introduces learnable curvature in positional embeddings, head-specific scaling in the attention mechanism, hyperbolic layer normalisation, gradient clipping, geodesic regularisation and layer scaling for training stability. These are specifc for Vision Transformers 
                4: Our model extends the standard self-attention mechanism into hyperbolic space using mobius operations and hyperbolic distance calculations to allow usnto capture complex relationship in visual data more effectively 

Methodology
    - Code Availability: Code is available at https://github.com/hyperbolicvit/hyperbolicvit

    - Hyperbolic Geometry Preliminaries
        > Hyperbolic space is chraracterised by constant negative curvature and it embeds hierarchical and complex structures common in visual data
        > Poncare ball model is used due to its computational convenience and suitablility for representing image data structures
        > Poincare ball model:
            - The n-dimensional Poincare ball model is defined as the manifold:
                D^n = {x in R | |x| < 1}, |.| is euclidean norm 
            - The Riemannian metric tensor g_x of this manifold is given by:
                g_x = (l_x)^2 g^E , with l_x = 2/( 1 - |x|^2 )
                where g^E is the Euclidean metric tensor, and the l_xis the conformal factor that scales the Euclidean metric to account for the curvature of hyperbolic space
        > Mobius Operations
            - Mobius transformations is utilised to adapt Vision Transformer components to hyperbolic space
            - These operations are essentual for processing vectors within the Poincare ball model
            1: Mobius Addition
                - For vectors x,y in D^n, the mobius addition x + y is defined as 
                    > x + y = \frac{(1+ 2<x,y> + ||y||^2 )x + (1 - ||x||^2)y}{1 + 2<x,y> + ||x||^2 ||y||^2}
                    > <.,.> denotes Euclidean inner product 
                        - For vectors x =  (x_1 ... x_n) and y = (y_1 ... y_n), <x,y> = sum[i=1:n] (x_i)(y_i) = x . y (dot product)
                        - The inner product encodes fundamental geometric concepts:
                            1: Length/Norm: ||u|| = √⟨u, u⟩
                            2: Angle: cos θ = ⟨u, v⟩ / (||u|| ||v||)
                            3: Orthogonality: u ⊥ v if and only if ⟨u, v⟩ = 0
                            4: Distance: d(u, v) = ||u - v||
                - Properties:
                    1: if |x| < 1 & |y| < 1, then |x+y| < 1 
                    2: Commutative : x + y == y + x
                    3: Associative: (a + b) + c == a + (b + c)
                    4: Identity Element 0: a + 0 = a 
                    5: Inverse of a is -a : a + -a = 0 
                - Mobius addition generalises vector additioni to hyperbolic space, ensuring the result remains within manifold 
                - Physical Interpretation:
                    > In special relativity, In special relativity, if you have two velocities v₁ and v₂ (as fractions of light speed c), their relativistic sum isn't simply v₁ + v₂. 
                    Instead, it follows the Mobius addition formula, ensuring the combined velocity never exceeds the speed of light.
                - Mobius Addition naturally emerges when working with distances and transformations in Poincaré disk model
            2: Mobius Scalar Mulitplication 
                - For a scalar r in R and a vector x in D^n, Mobius scalar mult r * x can be defined as 
                    > r * x = tanh( r tanh^-1 ( ||x|| ) ) \frac{x}{||x||}
                - Properties:
                    1: Distributivity over Mob Addition: r * (a + b) = (r * a) + (r * b)
                    2: Associativivity with real multiplication: (rs) * a = r * (s * a)
                    3: Identity Element 1: 1 * a = a 
                    4: Zero Annihilation: 0 * a = 0
                    5: Sign Behaviour: a * -1 = -a (Mobius Inverse) 
                - This operation scales a vector while preserving its direction and ensures the scaled vector remains within the Poincare ball.
                - In the Poincaré disk model, Mobius scalar multiplication corresponds to scaling along hyperbolic rays from the origin. The operation:
                    1: Preserves the direction of hyperbolic geodesics through the origin
                    2: Scales hyperbolic distances in a non-linear way
                    3: Maintains the conformal structure of the hyperbolic plane
            3: Mobius Matrix-Vector Mulitplication
                - Given a matrix W in R^(m x n) and a vector x in D^n, Mobius matrix-vector multiplication mv_mult(W, x) is defined as:
                    > mv_mult(W, x) = tanh( \frac{||Wx||}{||x||} tanh^-1(||x||) ) \frac{Wx}{||Wx||}, if x != 0 
                    > Or mv_mult(W,x) = (W_1,1 * x_1) + ... (W_1,n * x_n)
                - Mobius matrix-vector multiplication extends matrix-vector operations to hyperbolic geometry, providing a way to perform linear transformations in non-Euclidean spaces while 
                preserving the underlying hyperbolic structure.
                - This operation extends Mobius scalar multiplication to linear transformations, allowing us to apply linear layers within hyperbolic space 

            4: Mobius Concatenation
                - To combine multiple vectors x_1, ... x_n in D^n, we use Mobius concatenation:
                    MULT[j=1:n]x_j = x_1 * x_2 ... * x_n 

        - Hyperbolic Neural Network Components 
            > Mobius operations is incorporated into neural network components to enable ViTs to operate within hyperbolic space
            1: Hyperbolic Linear Layers 
                - Traditional layers (h = Wx + b) can be adapted to hyperbolic space using Mobius matrix-vector multiplication and addition:
                    > h = mv_mult(W,x) + b
                    > x in D^n, W in R^(m x n), b in D^m
            2: Hyperbolic Activation and Normalisation
                - Activation functions and normalisation are applied in the tangent space at the origin to leverage familiar Euclidean Operations
                    1: The logarithmic map log_0 : D^n -> T_0D^n maps points from the manifold to the tangent space
                        > log_0(x) = \frac{2tanh^-1(||x||)}{||x||} x
                    2: The exponential map exp_0 : T_0D^n -> D^n brings points back to the manifold
                        > exp_0(v) = tanh(\frac{||v||}{2}) \frac{v}{||v||}
                - with these maps, we define hyperbolic versioins of the ReLU activation and Layer Normalisation:
                    > ReLU^D(x) = exp_0(ReLU(log_0(x)))
                        * Can potentially do something similar to GeLU 
                    > LayerNorm^D(x) = exp_0(LayerNorm(log_0(x))) 
                - This approach allow us to apply standard activation and normalisation techniques within hyperbolic space 
        - Hyperbolic Layer Scaling 
            > To stabilise residual connections in hyperbolicnspace, we introduce a learnable scaling parameter \beta 
                X' = X + \beta * O, where X is the input, O is the output and \beta in R scales the residual contribution 

    - Hyperbolic Vision Transformer Architecture
        - Hyperbolic geometry is integrated into the ViT architecture by modifying key components to operate within hyperbolic space 
        1: Learnable Hyperbolic Positional Embedding 
            - Positional Embeddings are adjusted using a learnable curvature parameter c to represent positional information in hyperbolic space better:    
                > E_pos = c * E^0_pos, where E^0_pos is in D^(1 x N x E) are the initial positional embedding
                > X = X + E_pos
                * IP uses sine/cosine embeddings. How can this be applied here? 
        2: Hyperbolic Self-Attention Mechanism 
            - Self Attention mechanism is extended to hyperbolic space using Mobius operations and hyperbolic distance computations
                > Symbols for equations:
                    B : Batch size 
                    H : number of attention heads 
                    N : sequence Length
                    D : Head Dimenstion, where D = E/H and E is the embedding dimension 
                    c: Curvature parameter of the Poincare ball model 
                    \epsilon : small constant for numerical stability (1 * 10^-15)
                    \delta : small constant for clamping (1 * 10^-7)
                    p : DropConnect probability 

            - Given input embeddings X in D^(B x N x E) and using hyperbolic linear layers , we compute:
                1: Queries Q = mv_mult(X,W_Q) + b_Q 
                2: Keys K = mv_mult(X, W_K) + b_K
                3: Values V = mv_mult(X, W_V) + b_V
                > These values are then reshaped to accommodate multiple attention heads 
                    - Q, K, V in D^(B x H x N x D)
            
            - Hyperbolic Distance Computation 
                > The hyperbolic distance between a query Q_(b,h,i) and key K_(b,h,j) is computed using the distance function in the Poincare ball model:
                    d_D( Q_(b,h,i) , K_(b,h,j) ) = cosh^(-1) ( 1 + \frac{2c || Q_(b,h,i) + (-K_(b,h,j)) ||^2 }{ (1 - c|| Q_(b,h,i) ||^2 ) (1 - c|| K_(b,h,j) ||^2 ) + \epsilon}  )
                > To ensure numerical stability, we clamp the argument of cosh^(-1):
                    A_(b,h,i,j) = max( above_arg , 1 + \deta )
                > then distance is computed as:
                    d_D( Q_(b,h,i), K_(b,h,j) ) = log( A_(b,h,i,j) + sqrt( A_(b,h,i,j)^2 - 1 ) )
            
            - Attention Scores and Weights
                > Using the computed distance, attention scores with a head-specific scaling factor \alpha_h is calculated:
                    Att_(b,h,i,j) = - \frac{d_D( Q_(b,h,i), K_(b,h,j) )^2 }{ \alpha_h sqrt(D) }, where D is head dimension 
                > Attention weights are obtained by normalising the scores using the softmax function: 
                    \alpha_(b,h,i,j) = \frac{ exp(Att_(b,h,i,j)) }{ sum[k=1:N] exp(Att_(b,h,i,k)) }, where N is sequence length 
            
            - Output Computation
                > Each attention head produces an output by aggregating the value vectors, weighted by the attention weights:
                    O_(b,h,i) = Mob_sum[j=1,N]( mob_mult( \alpha_(b,h,i,j), V_(b,h,i,j) ) ), where N is seq length (note: * and mob_mult is used inter-changably by mood lol)

                > The outputs from all heas are combined using Mobius Concatenation and transformed with a final linear layer:
                    O_(b,i) = mv_mult( W_O, Mob_sum[h=1:H](O_(b,h,i)) ) mob_add b_O, where H is the number of attention heads (note: + and mob_add is used inter-changably by mood lol)
                
                > DropConnect regularisation is applied to the attention weights during training with probability p [Wan 2013 to know more]
            
            - Residual Connection with Layer Scaling 
                > As in the Hyperbolic Linear Layer, residual connections with a learnable scaling parameter \beta is used 
                    X' = X + \beta * O 

        3: Hyperbolic Feed-Forward Network 
            - FF network in hyperbolic space consists of 2 hyperbolic linear layers with a hyperbolic ReLU activation in between:
                H_1 = mv_mult(X', w_1) \mob_add b_1
                H_2 = ReLU^D(H_1)
                H_3 = mv_mult(H_2, w_2) \mob_add b_2
            - This is then followed by a residual connection and hyperbolic layer normalisation:
                X'' = LayerNorm^D(X' \mob_add \beta \mob_mult H_3 )

    - Optimising in Hyperbolic Space 
        > Optimising NN in hyperbolic space presents unique challenges due to its non-euc nature 
        > Several techniquees is employed to ensure stable and effective training 
        
        1: Gradient clipping
            > This is done to preven large gradients from destabilising training (better way to do this? probably not)
            > Gradient is clipped in the tangent space 
                grad_clipped = grad_\theta (if || grad_\theta || <= v) else v \dot_product \frac{grad_\theta}{ || grad_\theta|| } 
                    * ||.|| is euc-norm is tangent space and v is clipping threshold 
        
        2: Riemannian Adam Optimiser
            > We utilise the Riemannian Adam optimiser (an Adam optimiser that is adapted for Riemennian manifolds)
            > Update rule:
                \theta_(t+1) = exp_{\theta_t}(-\miniscule_t \dot_product  \fraction{ m_t^{clipped} }{ sqrt(v_t^{clipped}) +\epsilon } )
                * \miniscule_t is the learning rate 
                * m_t^{clipped} and v_t^{clipped} are the first and second moment estimates 
                * exp_{\theta_t}is the exponential map at \theta_t 
        
        3: Geodesic Distance Regularisation 
            > Regularisation term based on geodesic distance is introduced to enchance class separation and encourage meaningful representation 
            > L_{geo} = \lambda_{reg} \dot_product Expected_(i,j)[ (1 - \delta_(y_i,y_j)) \dot_product d_D(x_i,x_j)]
                * \delta_(y_i,y_j) is the Kronecker delta function which indicates whether samples i and j belong to the same class 
                * Total loss objective is L_{cross_entropy} + L_{geo}
        
        4: Layer scaling and Attention Scaling 
            > Introducing learnable scaling parameter \beta in residual connections and head-specific scaling factor \alpha_h in the attention mechanism 
            helps control the magnitude of the updates
            > it allows each attention head to adapt its sensitivity to hyperbolic distances 
        
        5: Parameter Initialisation 
            > Weights are initialised using Xavier uniform initialisation for hyperbolic space [Leimeister & Wilson 1029 to know more, not really impt] 
            > All manifold parameter are initialised within the Poincare ball to ensure valid representation and stable training 
    
    - Limitation 
        > While it gives be better hierarchical data handling, it incurs high computation costs due to complexity of hyperbolic operations 
            - But enhanced representation capabilities and scalability justify this trade-off 
            - Better computation equipments in the future can mitigate this too 
        > Additionally, Mobius op can be approximated to reduce complexity 
            - maybe can use PCA to reduce the vector dimensions 
            