Firstly understand what the project is trying to do

Paper Notes:

Paper cites 2 primary challenges faced in ICL in language and vision

	1: Given limited data, need appropriate inductive biases in observation and action representations for efficient learning in 3D space
	2: Given inefficiency and labour cost of manual collectiong of robotics data, need a means to easily collect training data in a scalable way

	First challenge is addressed by introducing novel graph-based representation that integrates demonstartions, current point cloud observations, and the robot's actions within a 
	unified graph space
		Then In-Context Imitation Learning becomes a diffusion-based graph generation problem, enabling demonstartions and observations to be 
		interpreted effectively in order to predict the robot's actions

	2nd challenged addressed, we get the model's weight to encode a more general, task-agnostic ability to interpret and act upon the given context.	
	This is unlike traditional BC, which model weights encode policies for a specific set of tasks
	
		By doing this, model can be trained on pseudo-demonstrations (set of demos for semantically consistent tasks)

Results:
	IP can learn vairous everyday tasks, whilst achieving higher task success rates than SOTA baselines trained on the same data
	IP is also able to genearlise to object geometries unseen from the test-time demos
	Performance improves as more data is generated and used for simultaneous training 
	IP achieves cross embodiment transfer from human-hand demos to robot policies
	IP achieves zero-shot transfer to language-defined tasks without needing large language-annotated datasets

Contributions
	1: cast ICL as diffusion-based graph generation problem
	2: Show that model can be trained using procedurally generated pseudo-demos
	3: Evaluate in sim and RL across various everyday tasks

Related Works:

	1: In-Context Learning:
		Allows models to adapt to new tasks using a small number of examples, without requiring explicit weight updates/retraining
		ICL has been applied to enable robots to rapidly adapt to new tasks by using 
			- foundation models, 
			- finding consisttent object alignments, 
			- identifying invariant regions of the state space 
			- directly training policies aimed at task generalisation
			- Cross embodiment transfer 
		However challenges remains in achieving generalisation to tasks unseen during training and novel object geometries 
			- In a sense, no creativity, or more so no meta-knowledge or planner that is able to utilise current knowledge/policies to generate new knowledge/policies

		IP currently addresses this by leveraging simulated pseudo-demos to generate abundant and diverse data, while its structured graph representation ensures that this data is utilised 
		efficiently 
			- is there a way to make it more general, such that it can be done in test-time (like deliberation) efficiently
			- Instead of making pseudo-demos in training time, we do a n-shot task-specific pseudo demo, and have a general model learn on it. With this, although inference time increases,
			training becomes more efficient, and policy is more task-specific.
			- The load 'off' training could be better used to develop a more general AI model. Right now, model achieves generality in object geometries and task by doing pseudo demos of 
			diverse objects on diverse task. If we 'tune' the task-specific obj geometry during inference time, and opt to use a general object during training time for pseudo demos, 
			our AI model can focus on achieving task generality, assuming that the nature of the task is not object specific? 
			The biggest question underlying this is whether the task is related to obj gemoetry. I believe this is not the case (majority of the time, links to the super hand project). 
			This assumption also overcomes the unpredictability of object geometry during test time. 
			- Perhaps some sort of entropic measure can be used to determine if a pseudo-demo is needed during inference time. This combines the current approach of having the model 
			be generic to task and obj gemoetry. During inference time, say if entropic measure is too high, we rollout pseudo demos to retrain the model such that it adapts to 
			the current obj geometry

	* A lot of the above points still doesnt make sense. Firstly, what does it mean to rollout and retrain during test time (cont learning)?
	* Secondly, how will the inner model be structured? Its a good idea but more thought needs to be put into it. 
	* I believe that Generative Adversarial IL might work for object geometries for empowerment measures? not sure about this one


	2: Diffusion Models
		Has the ability to iteratively refine randomnly sampled noise through a learned denoising process, ultimately generating high-quality samples from the underlying distribution.
		Utiliseed to create:
			1: Image augmentations to help robots adapt to diverse environments
			2: generating 'imagined' goals/subgoals for guidiung policies
			3: learning precise control policies
		This work proposes the use of diffusion models for graph generationg, enabling structured learning of complex distributions
		- The good thing here is we are using hetero graphs, which provides flexibility. If we do some clustering algo to 'categorise' the point cloud obserfvations to the 
		respective objects, it migh result in more consistent representation across task. 
			> Perspective/Orientation invariances might emerge if task representation during pseudo-demos remains task-specific. ( this is alread done by PointNet++ and Occ net lol )


	3: GNNs
		GNNs have been used in robotics to:
			- obtain RL policies
			- Learn affordance models for skill transfer
		This project builds on top of these applications by studying structured graph representations for ICIL, enabling learning of the relationship between demos, obs and actions


Overview of Problem:
	Goal: Robot to complete a novel task immediately after the provided demos. 
		- During test time,  >=1 demos given to define the context
		- The context defined is then used by the trained IP network with the current point-cloud observation to infer actions suitable for closed-loop reactive control
		- Enables instasntaneous policy acquisition w/o extensive real-world data collection or training.

	Achieved through
		- Structured graph representation
		- a learned diffusion process
		- An abundant source of diverse simulated pseudo-demos	

Problem Formulation:
	Robot actions a has 2 parts:
		- End-effector displacement SE(3) (when time scled, corr to velocities)
		- Binaryn open-closed commands for gripper	   

	Observation at timestep t has 3 parts
		- Segmented point clouds P
		- current end-effector pose in the world frame W SE(3)
		- Binary gripper stsate 

	Goal is to find a probability distribution, prob of taking action from time t to t+T, given current observation and N pseudo-demos of length L. 

	* Demo during test time defines the task (or context C). These demo are unlike pseudo-demos which are used during training time


Graph Representation:
	Aim is to cpature key elements of the problem and introduce appropriate inductive biases.
	Uses hetero-graph that jointly expresses context, current obs, and future actions 
		* (capturing complex r/s beteween robot & env; and ensure that relevant info is aggregated and propagated in a meaningful manner)
		- Each graph consist of 2 types of nodes, (robot state, local geometries of objects)
		- Each graph can be thought of as a snapshot of the current observation, and demos are graphs 'stitched together'

	> Local Representation
		Observation at time step t is expressed as a local graph, consisting of the current Point Cloud and current end-effector pose in the world frame and gripper state
			Then M points from the current Point Cloud is sampled using the Furthest Point Sampling Algorithm
			Points are then used to encode local geometries around them using Set Abstraction layers
			Feature vectors and positions are obtained from current point cloud -- {F,P}M = phi(P) 
				* phi(P) is separately pre-trained as an implicit occupancy network, ensuring that {F,p} describes local geometries
			Gripper state is then represented in the same format {F_g,p_g}6 by 
				1: rigdly transforming key points (p_kg) on the end-effector p_g = T x p_kp.
				2: assigning embeddings to p_g that encode node distinctions and gripper state information F_g = [f, phi_g(s_g)]
		Finally, scene and gripper nodes are linked with directional edges
			Each edge is assigned edge attribute e that represents relative positions in Cartesian space
			* Each edge e_ij = sin(2^0 * pi * (p_j-p_i), cos(2^0 * pi * (p_j - p_i) ... sin(2^(D-1) ... , cos(2^(D-1)...)
	
	> Context Representation
		As mentioned, a sequence of graphs, interleaved with actions, defines a trajectory within context C. 
		Interleaving is done by 
			1. linking gripper nodes across time to represent their relative movement 
			2. Connecting all demos gripper nodes to the current ones to propagate relevant info (why?)
		This enables the graph to efficiently handle any number of demos, regardless of length; whilst ensuring that the number of edges grows linearly
		This results in context, G_c(G_curr, {G_demo}), enabling a structured flow of information between context and curr obs.

	> Action Representation
		To express future actions a = (T,a_g) within the graph representation, a local graph is constructed as if the actions were executed and the gripper moved: G_a_l(P, T_WE x T_ea, a_g)
		This allows 'imagining' spatial implications of actions
		* Effectively, actions are fully described within positions and features of the nodes of local graphs
		To represent actions as relative movements from the current gripper pose, edges are added between current and future gripper nodes with position-based embeddings that represent 
		relative movement between subsequent timesteps
			* These edges propagate information from the context and the current observation; and propagates it to nodes representing the actions, enabling effective reasoning about the 
			robot actions by ensuring the observations and actions are expressed in the same graph space


Learning Robot Action via Graph Diffusion
	To utilised graph representation effectively, ICIL can be framed as a graph generation problem
	The aim is to learn a distribution over previously described graphs using a diffusion model
	Approach involves forward and backward Markov-chain processes, where graph is altered and reconstructed in each phase.
	At test time, the model iteratively updates only the parts of the graph representing robot actions
		*This implicitly models the desired conditional action probability	
	
	> TRAINING
		Training includes 2 processes: 
			1) Forward process where noise is iteratively added to the samples extracted from the underlying data distribution 
				Noise altered graph is constructed by adding noise to the robot actions 
			* Could consider using hyper graph like in Robotics SLAM, where 'space' or hyperedges represents uncertainty?
				* Variance schedule controls how noise is added throughout the diffusion steps
					** Higher b_k = more noise introduce
					** b_k usually starts small and increases over time, allowing for a gradual transition from the og data distribution to a complete noise distribution
					** factor sqrt(b_k) applied to prev actions ensures that the expected value of the distribution remains consistent, preventing a diverging diffusion process
					** b_k (or its growth) can be tuned to control how quickly the diffusion process transforms the og graph ot noisy one
			2) Reverse Process, where the aim is to reconstruct the original data sample from its noise-altered state, utilizing a parameterized model p(G^k-1 | G^k)
				Intuitively, model needs to learn how the gripper nodes representing the robot actions should be adjusted, st the whole graph moves closer to the true data distribution q(G))	
				Formally the de-noising process is G^k-1 = G(G^a_l(.) + N(0,VI), G_c)
				Within the G^a_l(.), . = alpha (a^k - rho e(G^k,k))
				e(G^k,k) can be interpreted as effectively predicting the gradient field, based on which single noisy gradient descent step is taken
					* Since actions is represented as collections of nodes with their assoc positions p and features, which in turn depend on the binary gripper actions, the gradient field e(.) 
					has 2 components : e = [dp, da_g] 
						> da_g can be used directly in the diffusion process.
					A set of dp predictions is an over-parameterisation of a gradient direction on the SE(3) manifold that requires additional steps to compute a precise denoising update
						However this can result in a large translation dominating a small rotation (and v.v), preventing precise learning for both components
						To address this, denoising direction is represented as a combination of centre-of-mass movement and rotation around it
						Effectively, this decouples the translation and rotation predictions, while keeping computation within Cartesian space 
							[dp_t, dp_r] = [t^0 - t^k, R^0 x p_kp - R6k x p_kp], with dp = dp_t + dp_r representing the flow
						e(.) is then learnt by making per-node predictions (e^k in R^7) and optimiseing the variational lower bound of the data likelihood, which is equivalent to minimising 
						MSE in e(.)
				As our parameterized model, a hetero graph transformer is used to update features of each node in the graph, F_i as [Masked Label Prediction.txt ]
					F'_i = W1 F_i + sum( att_ij (W2 Fj + W5 e_ij) )
					att_ij = softmax( (W3 F_i) (W4 F_j + W5 e_ij)/sqrt(d) )
					Here W_i represents learnable weights
					Using this attention mechanism allows for selective and informative information aggregation which is propagated through the graph in a meaningful way, while ensuring 
					that memory and computational complexity scales linearly with increasing context length (N & L) 
					* Could use attractor to better 'represent' attention mechanism so that relevant information is being 'focused'
					More in appendix C

	> Deployment
		During test time, graph representation is created using action sampled from normal dist. and context, which is inferred from current observation and demos given
		Predictions on how gripper nodes should be adjusted is then made
		These predictions are then used to update the positions of these gripper nodes by taking a denoising step according to Denoising Diffusion Implicit Model:
			p^k-1_g = sqrt(a_k-1)p'^0_g + sqrt( (1-a_k-1)/(1-a_k) ) (p^k_g - sqrt(a_k)p'^0_g)
			* p'^0_g = p^k_g + dp_t + dp_r
			* p^(k-1)_g and p^k_g implicitly represents gripper poses at denoising time steps k-1 and k.
				** Since we know ground truth values of these sets of points (since we collect them), we can extract an SE(3) transformation that would align them using a SVD.
				T_(k-1),k = argmin(T_(k-1),k in SE(3)) | p^(k-1) - T_(k-1),k x p^k |^2
		Finally a^(k-1) is calcualted from a^k by transforming it to T_(k-1,k)


Infinite Pool of Data
	- Since the IC policy is not task specific, we require demos that are task-arbitary by nature.
	- These demos/data only need to account for consistency in the physics involve within the tasks, or more so the semantics
	- Data Generation:
		> Env is populated with obejcts from Shape Net
		> Pseudo-task are generated by randomnly sampling object-centric waypoints near or on the object
		> Pseudo-task are then performed with virtual movements, and occasionnal simulated gripping of the object is done by attaching objects to gripper
		> Orientation of objects and gripper is also randomised to generate more demos (the point of it all is the consistent semantics we want!)

	- Data Usage:
		> During training, we sample N pseudo-demos for a given pseudo-task, using N-1 to define the context, while the model learns to predict actions for the Nth
		> Although pseudo-demos are primary training data, additional data sources can be integrated in the same format. allowing the model to adapt to specific settings or handler noisier env/obs


Geometry Encoder 
	- This geometry encoder is used to identify which point cloud belongs to which objet's surface
	- It is used to represent observation of the environment as a aset of nodes
	- Formally, it encodes the dense point cloud into a set of feature vectors together with their associated positions as {F^i, p^i}i, i=[0,M], where M is the number of point cloud
		> Each feature Fi describes the local geometry around point p^i
		> This is ensured using a pre-trained occupancy network that consist of an encoder phi_e, that embeds local point clouds and a decoder psi_d which given an embedding and a query point is tasked to determine whether the query lays on the surface of the object 
			- psi_d( phi_e(P), q ) -> [0,1]
			- each local embedding is used to reconstruct only a part of the object, reducing the complexity of the problem and allowing it to generalise more easily.
	- We parameterized phi_e as a network composed of 2 Set Abstraction layers enhanced wth Nerf-like sine/cosine embeddings
		> It samples M centroids from the dense point cloud and embdeds the local geometries around them into feature vectors of size 512 
		> we express the positions of points as (sin(2^i pi (p_j - p_i)), cos( 2^i pi (p_j - p_i)) ...), i = [0:9], enabling the odel t cature high-frequency changes in position of the dense points and capture the local geometry more precisely 
	- psi_d is parameterized as an eight-layer MLP with residual connections, that uses the same Nerf-like embeddings to represent the position of the query point
	- When training instsant policy, we dont use the decoder and keep the encoder frozen


Training 
	- Training can be intuitively understood as a forward and backwawrd Markov chain diffusion process, where noise is added to the ground truth robot actions in the forward pass, and noise is removed from the graph space in the backward pass
	- Practically, training has 4 steps:
		1: Noise is added to the ground truth actions 
			> To add noise the the action expressed as (T_ea in Lie group SE(3), a_g in R), we first project T_ea to se(3) using a Logmap.
			> the result vectors are then normalised, with noise added to in as described in denosiing diffusion paper 
			> this is then un-normalised to extract the noisy end-effector transformation T^k_EA using Expmap 

		2: Noisy actions are used to construct our graph representation 
		3: network preducts how nodes representing robot actions need to be adjusted to effectively remove the added noise 
		4: the prediction and ground truth labels are used to calculate the loss function, and weights are updated accordingly


Network architecture
	- The Neural Network takes in as input the constructed graph representation, and predicts the gradient field for each gripper node representing the actions
	- These predictions are then used in the diffusion process allowing to iteratively update the grpah and ultimately extract the desired low-level robot action 
	- However, for computational efficiency and more controlled information propagation, 3 separate networks are used to update relevant parts of the graph in sequence
		> F(G^k) = f3(
						G(
							f1(G^a_l), 
							f2(
								G_c(f1(G^t_l)),
								{f1(G^[1:L]_l)}^N_1
							)
						)
					)
		> f1 operates on local subgraphs G_l and propagates initial information about the point cloud observations to the gripper nodes
		> f2 additionally propagates information through the demonstrated trajectories and allows all the relvant information from the context to be gathered at the gripper nodes of the curentsubgraph 
		> f3 propagates information to nodes in the graph representing the actions
	- Using such a structured and controlled propagation of information through the gtraph, together with the learnable attention mechanism allows the model to continuously aggregate relevant information from the context and make accurate predictions about the actions 
		> It also resuts in a clear and meaningful bottleneck in the network with all relevant information from the context aggregated in a specific set of nodes f2(.)
		> This bottleneck representation could be used for retrieval or to switch modalities (eg to language) via a smaller annotated dataset and a contrastive objectives
	- f1, f2 and f3 are hetero-graph transformers with 2 layers and a hidden dimension of size 1024 (16 heads each with 64 dimensions)
		> Since hetero graph are used, each node and edge type are processed with separate learnable weights and aggregated via summation to produce all node-wise embeddings 
		> This can be understood as a set of cross-attention mechanism, each repsonsible for rocessing different parts of the graph representation 
		> layer normalisation (lay-norm) is done between every attention layer, with additional residual connections added to ensure good propagation of gradients throughout the Network
		> Finally, features of the nodes representing robot actions are processed with a 2 layer MLP equipped with GeLU activations (GeLU) to prduce the per-node denoising directions


Data Generation
	> Process
		- firstly, the scene is populated with objects with which the robot will interact
			> This is done by sampling 2 objects from the ShapeNet dataset and placing them randomnly on a plane
		- then a pseudo-task is defined by sampling a sequence of waypoints on or near those objects
			> number of waypoints is also randomnly selected (2-6), inherently modelling various manipulation tasks.
		- 1 or more waypoints is assigned to change the gripper state, mimicking the rigid robotic grasp and release 
		- a starting pose is then sampled for the gripper, where a mesh of a Robotiq 2F85 gripper is initialised 
			> by moving the gripper between the aforementioned waypoints and attaching/detaching the closest object to it when the gripper state changes, we create a pseudo-demo 
		- to further increase the diversity in pseudo-tasks, dirrent interpolation strategies between the waypoints (linear, cubic, interpolating while staying on a spherical manifold)
		- gripper poses and segmented point cloud obs are then recorded using PyRender and 3 simulated depth cameras
		- spacing between subsequent spaces are kept constant and uniform at 1cm and 3 degrees 
		- moving objects to different poses, choosing a different starting gripper pose and repeating the process results in several pseudo-demos for the same pseudo-task
			> we dont need to ensure that these generated trajectories are dynamically/kinematically feasible since env dynamics and taask specs are inferred from context at inference
	> Bias Sampling 
		- sampling is biased to favour waypoints resembling common tasks like grasping and pick-and-place to facilitate more efficient learning of common skills 
		- this does not require creating dynamically feasible trajectories but rather involves designing sampling strategies for waypoints that loosely approximate these tasks 
			> for instance by selecting a specific part of an object, moving the simulated gripper to that location, and closing the gripper, we can simulate a grasping task
		- such sampling strategies are designed for common tasks (grasping, pick-and-place, opening/closing)
		- half of pseudo-demos are generated using these strategies, well the rest are kept random
	> Data Augementations 
		- generated trajectories are augmented to facilitate the emergence of recovery behaviour of the learnt policies 
		- firstly, for 30% of the trajectories, local disturbance associated with actions that would bring the robot back to the regference trajectory are added (NeRF-palm)
		- secondly, for 10% of the data points, gripper's state (open/close) are changed purposely 
			> this is found to be crucial as without it the policy will never try to re-grasp an object after initially closing the gripper 

Implementation Details
	> Demo Processing
		- Demo trajectories are downsampled to a fixed length (L = 10)
		- Demonstrations are recorded at a rate of 25Hz and 10Hz in simulation and the real world
			> Lower rates in the real world is caused by the simultaneuos segmentation of objects of interest by Xmem++ (Xmem++)
		- start and end of trajectories are then included and all the waypoints where gripper state changed are included too
		- waypoints where gripper sufficiently slowed down are also included, as they indicate important trajectory stages 
		- if current number of waypoints are less than L, intermediate waypoints between the already extracted ones are added 
	> Data Augementations
		- to achieve robust policies, it is crucial to randomise current observations and subsequent actions during training 
			> the network can easily overfit to binary gripper states (if open, it stays opened and v.v)
				- this is tackled by doing a 10% poisson dist, where the gripper state that is used as input to the model is flipped 
			> doing this greatly increase the robustness of the resulting policies 
			> during pseudo-demos generation process, local perturbation to the pose of the ripper is added, and point cloud observations and actions are adjusted accordingly
				- this further increase robustness and enable recovery behaviour 
				- pc obs and actions are adjusted to 'recover' the robot path 
	> Normalisation 
		- all outputs of the model is normalised to [-1,1], which is found to be crucial 
		- to do this, we manually define the maximum end-effector displacement between subsequent action predictions to be no more tahn 1 cm in translation and 3 deg in rotation 
			> noisy actions are also clamped to be within this range 
		- thus, the flow prediction is caped to be at most twice the size of this range 
			> knowing this, d_p'_t and d_p'_r (translation and rotation) are normalised between -1 and 1 independently, enabling efficient network training 
		- for gripper state canges, this is done easily since they are expressed as binary states {0,1}
		- we dont normalise the position of the point cloud observation but rather rely on the sine/cosine embeddings, a strategy found be to sufficient 
	> Point Cloud Representation 
		- segmented point cloud observations og object of interest in env as observations 
		- those segmented point clouds dont include points on the robot or other static objects such as the table or distractors 
		- in practice, point clouds are downsampled to contain 2048 points and they are expressed in the end-effector frame as T_ew x P to achieve stronger spatial generalisation capabilities 
		- point clouds are then processed with a geometry encoder producing M=16 nodes used to construct our devised graph representation 
	> Action denoising 
		- when updating T_ea during our denoising process, we use calculated T_k,k-1 and calculate the transformation representing end-effector actions during the denoising process as T^k-1_ea = T_k,k-1 x T^k_ea 
		- these actions are then used to construct a graph representation that is used in the next denoising step
		- in practice, since we express point cloud obs in the end-effector frame, we apply the inverse of these actions to the M points representing the scene and construct local graphs of actions as: 
			> G^a_l(T^(-1)_ea X P^t, T^t_we, a_g)
			> since there are no absolute positions in the graphs, this is equivalent to applying the actions to te gripper pose T^t_we, but it allows us to recompute the geometry embeddings of the point clouds at their new pose, better matching the ones from demos 
	> Simualted Experimental Setup
		1: We generated all demos using only Cartesian Space planning, disregarding all demos that were generated using an RRT-based motion planner 
			- this is so as to ensure taht demos did not have arbitary motions that will not be captured by our observations of segmented point clouds and end-effector poses
		2: The orientation of the objects in the environment is restricted to be within [-pi/3, pi/3] 
			> this is to match the distribution of object poses to the one present in our generated pseudo-demos
			> also to ensure taht most tasks can be solved without complex motions requiring motion planners
	> Failure Cases
		- ...

TLDR for implementating paper:
	1: Generate pseudo demos for X tasks
		1.1: Define X tasks 
		1.2: Come up with generation strategy for these X tasks, done via sampling strategies on waypoint and gripper states 
	2: Constructing graph representations from observations
		2.1: constructing local graphs
		2.2: constructing context graphs 
		2.3: constructing action graphs 
	3: Defining Neural Network architecture 
	4: Construct RLBench tasks 
	