Occupancy Networks: Learning 3D Reconstruction in Function Space

Abstract
    - In 3D reconstruction, there does not exist a canonical representtion which is both computationally and memory efficient, while allowing for ]
      representation in high-resolution geometry of arbitarary topologies 
    - Many SOTA learning based 3D-Recon. approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain 
    - Occupancy networks is a new representation for learning-based 3D-Recon. methods
        > it implicitly represent the 3D surface as the continuous decision boundary of a depp NN classifier 
    - In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint 
    - It is validated that the 3D representation can efficiently encode 3D structure and can be inferred from various kinds of input 
    - Experiments demonstrates competitive results from a variety of challenging task 


Introduction 
    - Learning based approach for 3D-Recon is better than traditional multi-view stereo algorithms as it is able to encode rich prior information about the 
      space of 3D shapes which helps resolve ambiguities in the input 
    - However, there has been challenge in achieving a 3D representation that is both memory efficient and that can be efficiently inferred from data
    - Existing representation can be categorised in 3 categories:
        1: Voxel-based representation
            > this is a straightforward generalisation of pixels ito the 3D case 
            > However, memory footprint of this representation grows cubically with resolution, hence limiting naive implementation 
            > While it is possible to reduce memory footprints (with oc-trees for eg), this can lead to complex implementation
            > Existing data-adaptive algorithms are still limited to relatively small voxel grids 
        2: Point-based representation
            > This approach lacks connectivity structure of the underlying mesh and hence requires additional post-=processing steps to extract 3D geometry 
        3: Mesh-based representation 
            > Existing mesh representation are typically based on deforming a template mesh and hence do not allow arbitrary topologies 
        * Both 2 & 3 are limited in the number of points/vertices which can be reliably predicted using a standard feedforward network 
    - Occupancy Network is based on directly learning the continuous 3D occupancy function 
        > Instead of predicting a voxelised representation at a fixed resolution, we predict the complete occupancy function with a neural network f_theta
            - f_theta can be evaluated at arbitrary resolution, which drastically reduces memory footpring during training 
            - At inference time, we extract the mesh from the learned model 
                > this is done by using a simple multi-resolution isosurface extraction algorithm which trivially parallelises over 3D locations 
        > Contributions:
            1: Introduce new 3D representaion 
            2: Show how this representaion can be used for 3D-Recon from various input types 
            3: Experimentally validate that our approach is able to generate high-quality meshes and demostrates SOTA performance 


Related Works 
    1: Voxel Representations
        - Due to their simplicity, they are commonly used for discriminative and generative 3D tasks 
        - Early works considered 3D-recon from a single image using 3D CNNs which operate on voxel grids 
            > however, these are limited to small voxel grids due to memory requirements 
            > While recent works have been able to utilise larger grid size, they are only possible with shallow architectures and small batch size, 
              leading to slow training 
        - Later works looks at 3D-recon from multiple input views (>1 image)
            > One approach encode camera parameters together with input images in a 3D voxel representation and apply 3D conv to reconstruct 3D scenes from mult views 
                * Ji et al. and Kar et al. (authors)
            > Another approach introduces an architecture that predict voxel occupancies from multiple images, exploiting multi-view geometry constraints
                * Paschalidou et al. (author)
            > Other works applied voxel representation to learn generative models of 3D shapes 
                - Most of these methods either based on variational auto-encoder or generative adversarial networks (GAIL) 
                    * Eg in papers 4, 58, 75 in paper 
        - Due to high memory requirements of voxel representations, recent works have proposed to reconstruct 3D objects in multi-resolution fashion [28,67]
            > However resulting methods are often complicated to implement and require multiple passes over the input to generate the final 3D model 
                - Furthermore, they are still limited to small voxel grids 
        - For achieving sub-voxel precision, severak works have proposed to predict truncated signed distance fields (TSDF) where each point in a 3D grids 
          stores the truncated signed distance to the closest 3D surface point
            > However, this representation is usually much harder to learn compared to occupancy representations as the network must reason about distance 
              functions in 3D space instead of merely classifying a voxel as occupied or not 
            > This representation is still limited by the resolution of the underlying 3D grid too 
    2: Point representaion
        - An alternative representation of 3D geometry is given by 3D point clouds
        - [54,56] prioneered point clouds as a repreesntation for discriminative deep learning tasks 
            > They achieved permutation invariance by applying a fully connected NN to each point independently followed by a global pooling operation 
        - [17] introduced point clouds as a output representation of 3D-Recon 
        - However, unlike other approach, this approach requires additional non-trivial post-processing steps to generate the final 3D mesh 
    3: Mesh Representation 
        - Meshes have first been considered for discriminative 3D classification or segmentation tasks by applying convolutions on the graph spanned by the 
          mesh's vertices and edges
        - They have also been considered as output representation for 3D recon
            > however, most of these approach are prone to generating self-intersecting meshes 
            > furthermore, they are only able to generate meshes with simple topology, require a reference template from the same object class or 
              not being able to guarantee closed surface 
            > [43] proposed an end-to-end learnable version of the marching cubes algorithm [44]
                - But it is still limited by memory requirements of the underlying 3D grid and hence also restricted to small voxel resolution 
    - In contrast to the aforementioned approaches, occupancy net leads to high resolution closed surfaces without self-intersection 
        > it also doesnt require template meshes from the same object class as input 
        > this idea is related to classical level set [1,14,50] approaches to multi-view 3D-recon 
            * might be worth to check out these papers to better understand intuition of occupancy net 
            - However, instead of solving a differential equation, we use deep learning to obtain a more expressive representation which can be naturally
              integrated into an end-to-end learning pipeline 

Method 
    Occupancy Net : Introduction 
        - Ideally, we want to be able to reason about the occupancy at every possible 3D point (p in R^3) as opposed to only at fixed discrete 3D location (like in Voxel Rep.)
            > we call this reasoning function the occupancy function of the 3D object  o : R^3 -> {0,1}
            > This function can be approximated with a NN that assigns to every location p an occupancy probability between 0 and 1 
                * The NN mentioned above is equivalent to a NN for binary classification, but in this case we are interested in the decision boundary which 
                  implicitly represents the object's surface 
                - When network is used for 3D-recon based on obs of object (image/point cloud), inference must be conditioned on the input 
                    > A simple functional equivalence to achieve this 
                        - A function that takes an obs x in X as input and has a function from p in R^3 to R as output is equivalent to a function that
                          takes a pair (x,p) in R^3 x X as input and outputs a real number 
                        - This latter function can be simply parameterized by a NN f_theta that takes a pair (p,x) as input and outputs a real number 
                            > f_theta : R^3 x X -> [0,1]. This is known as the Occupancy Network 
    Training 
        - To learn the parameters \theta for the Occupancy Network, we randomnly sample points in the 3D bounding volume of the object under consideration
            > For the i-th sample points in a training batch, we sample K points p_ij in R^3, j = 1 ... K
            > The mini batch loss L_B is then evaluated 
                - L_B(\theta) = 1/|B| sum[i = 1:B] sum[j = 1:K] L(f_theta(p_ij, x_i), o_ij)
                    > x_i is the ith observation of batch B, o_ij denotes the true occupancy at point p_ij and L(.,.) is a cross entropy classification loss
        - The performance of the model depends on the sampling scheme that is employed for drawing the locations p_ij that are used for training 
            > In practice, it is found that sampling uniformly inside the bounding box of the object with an additional small padding yields the best results
        - Using occ-net's 3D representation can also be used for learning probabilistic latent variable models 
            > Towards this goal, an encoder network g_phi(.) that takes location p_ij and occupancies o_ij as input is introduced
                - this encoder predicts mean and std of a Gaussian dist q_phi(z|(p_ij, o_ij)_[j= 1:K]) on latent var z in R^L as output 
            > Then a lower bound to the negative log-likelihood of the generative model p((o_ij)_[j=1:K] | (p_ij)_[j=1:K]) is optimised:
                - L^gen_B (\theta, \phi) = L_B + KL(q_phi(.) | p_0(z) )
                    > p_0(.) is the prior distribution on latent variable z (typically Gaus) and z_i is sampled according to q_phi(.) 
    Inference 
        - For extracting the isosurface corresponding to a new obs given a trained occupancy network, we introduce Multiresolution IsoSurface Extraction (MISE)
            > MISE is a hierarchical isosurface extraction algorithm 
            > By incrementally building an octree, MISE allows for extraction of high resolution meshes from occupancy network without densely evaluating
              all points of a high-dimensional occupancy grid 
        - MISE is done by first discretizing the volumetric space at an initial resolution and evaluating the occupancy network f_theta(p,x) for all p in this grid
            > We mark all grid points p as occupied for which f_theta(p,x) >= \tau, where \tau is some threshold 
            > Next we mark all voxels as active if there are at least 2 adjacent grid points that have different occupancy predictions 
                - These are the voxels that will intersect the mesh if we applied the marching cubes algorithm at the current resolution 
            > Then, all active voxels are subdivied into 8 subvoxels and evaluate all new grid points which are introduced to the occupancy grid through this sub division 
            > Steps are repeated till a desired resolution is reached 
            > At final desired resolution, Marchin Cubes algorithm [44] is applied to extract an approximate isosurface 
                - {p in R^3 | f_theta(p,x) = \tau}
        - This algorithm converges to the correct mesh if the occupancy grid at the initial resolution contains points from every connected component of both the interior and exterior of the mesh 
            > It is hence important to take an inital resolution which is high enough to satisfy this condition 
            > In practice, an initial resolution of 32^3 is sufficient in most cases 
        - The initial mesh extracted by the Marching cubes algorithm can be further refined 
            > In the first step, we simplify the mesh using the Fast-Quadratic-Mesh-Simplification algorithm [20]
            > Then the ouput mesh is refined using first and second order (gradient) information
                - Towards this goal, we sample random points p_k from each face of the output mesh and minimise the loss: 
                    > sum[k =1:K](f_theta(p_k,x) - \tau)^2 + \lambda|| \frac{delta_p f_theta(p_k,x)}{|| delta_p f_theta(p_k,x) ||} - n(p_k) ||
                        - n(p_k) denotes the normal vector of the mesh at p_k 
                        - In practice \lambda = 0.01 
                        - Minimisation of the 2nd term uses second order gradient information and can be efficiently implemented using Double-Backpropagation [15]
                - This step removes the discretisation artifacts of the Marching Cubes approximation 
                    > This will not be possible if a direct prediction of a voxel-based representation is done 
                - Additionally, this approach allows efficient extraction of normals for all vertices of output mesh with simple backpropagation through occupation network 
                - In total, inference algorithm takes 3s per mesh 
    Implementation details 
        - Occupancy network is implemented using a fully connected neural network with 5 ResNet blocks and conditioning it on the input using conditional batch normalisation 
        - Different encoder architecture is exploited based on the type of input 
            > For point clouds and unconditional mesh generation, PointNet encoder is used (PointNet++ in instant policy)
            

        